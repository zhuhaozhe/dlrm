commit 49a10bc2a7f06466fff85626b03ef5d418742fc8
Author: zhuhaozhe <Haozhe.Zhu@intel.com>
Date:   Mon Jul 6 16:58:24 2020 +0800

    int8emb

diff --git a/aten/src/ATen/native/EmbeddingBag.cpp b/aten/src/ATen/native/EmbeddingBag.cpp
index 29cf2da..0b1ed79 100644
--- a/aten/src/ATen/native/EmbeddingBag.cpp
+++ b/aten/src/ATen/native/EmbeddingBag.cpp
@@ -135,6 +135,54 @@ void index_select_add<float>(const Tensor &select_indices,
   }
 }
 
+template<>
+void index_select_add<int8_t>(const Tensor &select_indices,  //input
+                             const Tensor &add_indices,  //offset2bag
+                             const Tensor &src,  //weight
+                             Tensor &output,
+                             const Tensor& offsets,
+                             bool include_last_offset) {
+  int64_t ddim = src.size(1);
+  auto src_data = src.data_ptr<qint8>();
+  auto select_indices_data = select_indices.data_ptr<int64_t>();
+  auto output_data = output.data_ptr<float>();
+  auto scales = src.q_per_channel_scales();
+  auto scales_data = scales.data_ptr<double>();
+  int64_t output_size = offsets.numel() - 1;
+  auto* offsets_data = offsets.data_ptr<int64_t>();
+  std::vector<int64_t> offsets_include_last;
+  if (include_last_offset) {
+    output_size = offsets.numel() - 1;
+  } else {
+    output_size = offsets.numel();
+    offsets_include_last.resize(offsets.numel() + 1);
+    std::memcpy(
+        offsets_include_last.data(),
+        offsets.data_ptr<int64_t>(),
+        sizeof(int64_t) * offsets.numel());
+    offsets_include_last[offsets.numel()] = select_indices.numel();
+    offsets_data = offsets_include_last.data();
+  }
+  at::parallel_for(
+        0, output_size, 1, [&](int64_t start_idx, int64_t end_idx) {
+      caffe2::pt_EmbeddingLookupIdx(
+          /*block_size=*/src.size(1),
+          /*output_size=*/end_idx - start_idx,
+          /*index_size=*/offsets_data[end_idx] - offsets_data[start_idx],
+          /*data_size=*/src.size(0),
+          /*input=*/reinterpret_cast<int8_t*>(src_data),
+          /*indices=*/select_indices_data + offsets_data[start_idx],
+          /*offsets=*/offsets_data + start_idx,
+          /*weights=*/nullptr,
+          /*scales=*/scales_data,
+          /*normalize_by_lengths=*/false,
+          /*out=*/output_data + start_idx * ddim
+        );
+  });
+
+}
+
+
 // This function fuses the following three fns:
 // index_select (using select_indices as the index)
 // mul (scaling by per_sample_weights)
@@ -373,7 +421,7 @@ _embedding_bag_cpu(const Tensor &weight, const Tensor &indices,
   auto offsets_arg = TensorArg(offsets, "offsets", 1);
   checkScalarType("embedding_bag", offsets_arg, kLong);
   auto weight_arg = TensorArg(weight, "weight", 1);
-  checkScalarTypes("embedding_bag", weight_arg, {kFloat, kDouble});
+  checkScalarTypes("embedding_bag", weight_arg, {kFloat, kDouble, at::kQInt8});
   int64_t offset_0 = offsets.data_ptr<int64_t>()[0];
   int64_t offset_n = offsets.data_ptr<int64_t>()[offsets.size(0)-1];
   TORCH_CHECK(offset_0 == 0, "offsets[0] has to be 0, i.e., the first sequence "
@@ -404,7 +452,7 @@ _embedding_bag_cpu(const Tensor &weight, const Tensor &indices,
   auto output = at::zeros(
       {include_last_offset ? offsets.size(0) - 1 : offsets.size(0),
        weight.size(1)},
-      weight.options());
+      weight.is_quantized() ? at::device(c10::kCPU).dtype(c10::kFloat) : weight.options());
 
   // To save compute, if we are going to go down the fast path case for the 'sum'
   // mode, we skip calculating offset2bag, since it is not going to be used.
@@ -434,15 +482,23 @@ _embedding_bag_cpu(const Tensor &weight, const Tensor &indices,
   }
 
   if (mode == MODE_MEAN || mode == MODE_SUM) {
-    AT_DISPATCH_FLOATING_TYPES(weight.scalar_type(), "embedding_bag_cpu", [&]() {
-      if (per_sample_weights.defined()) {
-        AT_ASSERT(mode == MODE_SUM);
-        index_select_scale_add<scalar_t>(
-            indices, offset2bag, per_sample_weights, weight, output, offsets, include_last_offset);
-      } else {
-        index_select_add<scalar_t>(indices, offset2bag, weight, output, offsets, include_last_offset);
-      }
+   if (weight.is_quantized()) {
+      AT_ASSERT(mode == MODE_SUM);
+      AT_ASSERT(!per_sample_weights.defined());
+      AT_ASSERT(weight.scalar_type() == at::kQInt8);
+      AT_ASSERT(weight.is_contiguous() && output.is_contiguous());
+      index_select_add<int8_t>(indices, offset2bag, weight, output, offsets, include_last_offset);
+    } else {
+      AT_DISPATCH_FLOATING_TYPES(weight.scalar_type(), "embedding_bag_cpu", [&]() {
+        if (per_sample_weights.defined()) {
+          AT_ASSERT(mode == MODE_SUM);
+          index_select_scale_add<scalar_t>(
+              indices, offset2bag, per_sample_weights, weight, output, offsets, include_last_offset);
+        } else {
+          index_select_add<scalar_t>(indices, offset2bag, weight, output, offsets, include_last_offset);
+        }
     });
+    }
     auto ret = apply_bag_size(offsets, indices, mode, output, bag_size);
     return std::tuple<Tensor, Tensor, Tensor, Tensor>(ret, offset2bag, bag_size, bag_size);
   } else { // MODE_MAX
diff --git a/aten/src/ATen/native/native_functions.yaml b/aten/src/ATen/native/native_functions.yaml
index 724ff91..53c548f 100644
--- a/aten/src/ATen/native/native_functions.yaml
+++ b/aten/src/ATen/native/native_functions.yaml
@@ -1098,6 +1098,7 @@
 - func: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False) -> (Tensor, Tensor, Tensor, Tensor)
   dispatch:
     CPU: _embedding_bag_cpu
+    QuantizedCPU: _embedding_bag_cpu
     CUDA: _embedding_bag_cuda
 
 - func: _embedding_bag_backward(Tensor grad, Tensor indices, Tensor offsets, Tensor offset2bag, Tensor bag_size, Tensor maximum_indices, int num_weights, bool scale_grad_by_freq, int mode, bool sparse, Tensor? per_sample_weights) -> Tensor
diff --git a/caffe2/perfkernels/embedding_lookup_idx.cc b/caffe2/perfkernels/embedding_lookup_idx.cc
index 1216c6b..8d37e18 100644
--- a/caffe2/perfkernels/embedding_lookup_idx.cc
+++ b/caffe2/perfkernels/embedding_lookup_idx.cc
@@ -74,6 +74,69 @@ static bool EmbeddingLookupGenericSlowIdx(
   return current == index_size;
 }
 
+//different from caffe2, pytorch quantized embedding bag get scales for "double" type
+template <
+    typename IndexType,
+    typename InType,
+    typename OutType,
+    bool IS_WEIGHT_POSITIONAL = false>
+static bool pt_EmbeddingLookupGenericSlowIdx(
+    const int64_t block_size,
+    const int64_t output_size,
+    const int64_t index_size,
+    const int64_t data_size,
+    const InType* input,
+    const IndexType* indices,
+    const int64_t* offsets,
+    const float* weights, // optional, can be null for sum reducer
+    const double* scales, // optional scale & bias params for uint8 input
+    bool normalize_by_lengths,
+    OutType* out) {
+  int64_t current = 0;
+  for (int m = 0; m < output_size; ++m) {
+    memset(out, 0, sizeof(OutType) * block_size);
+    if (current != offsets[m]) {
+      return false;
+    }
+    int64_t start_offset = offsets[m];
+    int64_t end_offset = (m == output_size - 1 ? index_size : offsets[m + 1]);
+    int64_t length = end_offset - start_offset;
+    for (int i = start_offset; i < end_offset; ++i) {
+      int64_t idx = indices[current];
+      if (idx < 0 || idx >= data_size) {
+        return false;
+      }
+#ifdef __GNUC__
+      if (current + 1 < index_size) {
+        __builtin_prefetch(input + block_size * indices[current + 1], 0, 1);
+      }
+#endif // __GNUC__
+
+      float w = 1.f, b = 0.f;
+      if (weights) {
+        w = weights[IS_WEIGHT_POSITIONAL ? i - start_offset : current];
+      }
+      if (scales) {
+        w = w * scales[indices[current]];
+      }
+
+      for (int j = 0; j < block_size; ++j) {
+        out[j] += w * input[block_size * indices[current] + j] + b;
+      }
+
+      ++current;
+    }
+    if (normalize_by_lengths && length) {
+      float scale = 1.f / length;
+      for (int j = 0; j < block_size; ++j) {
+        out[j] *= scale;
+      }
+    }
+    out += block_size;
+  }
+  return current == index_size;
+}
+
 // Proxy back to generic implementation
 #define EMBEDDING_IDX_SPECIALIZATION(                                                                 \
     IndexType, InTypeName, InType, OutType, IS_WEIGHT_POSITIONAL)                                     \
@@ -223,4 +286,146 @@ EMBEDDING_IDX_SPECIALIZATION(int64_t, uint8_t, uint8_t, float, true);
 
 #undef EMBEDDING_IDX_SPECIALIZATION
 
+//different from caffe2, pytorch quantized embedding bag get scales for "double" type
+#define PT_EMBEDDING_SPECIALIZATION(                                                                     \
+    IndexType, InTypeName, InType, OutType, IS_WEIGHT_POSITIONAL)                                     \
+  bool                                                                                                \
+      pt_EmbeddingLookupIdx_##IndexType##_##InTypeName##_##OutType##_##IS_WEIGHT_POSITIONAL##__base(     \
+          const int64_t block_size,                                                                   \
+          const int64_t output_size,                                                                  \
+          const int64_t index_size,                                                                   \
+          const int64_t data_size,                                                                    \
+          const InType* input,                                                                        \
+          const IndexType* indices,                                                                   \
+          const int64_t* offsets,                                                                     \
+          const float* weights,                                                                       \
+          const double* scales,                                                                    \
+          bool normalize_by_lengths,                                                                  \
+          OutType* out) {                                                                             \
+    return pt_EmbeddingLookupGenericSlowIdx<                                                             \
+        IndexType,                                                                                    \
+        InType,                                                                                       \
+        OutType,                                                                                      \
+        IS_WEIGHT_POSITIONAL>(                                                                        \
+        block_size,                                                                                   \
+        output_size,                                                                                  \
+        index_size,                                                                                   \
+        data_size,                                                                                    \
+        input,                                                                                        \
+        indices,                                                                                      \
+        offsets,                                                                                      \
+        weights,                                                                                      \
+        scales,                                                                                   \
+        normalize_by_lengths,                                                                         \
+        out);                                                                                         \
+  }                                                                                                   \
+  decltype(                                                                                           \
+      pt_EmbeddingLookupIdx_##IndexType##_##InTypeName##_##OutType##_##IS_WEIGHT_POSITIONAL##__base)     \
+      pt_EmbeddingLookupIdx_##IndexType##_##InTypeName##_##OutType##_##IS_WEIGHT_POSITIONAL##__avx2_fma; \
+  bool                                                                                                \
+      pt_EmbeddingLookupIdx_##IndexType##_##InTypeName##_##OutType##_##IS_WEIGHT_POSITIONAL(             \
+          const int64_t block_size,                                                                   \
+          const int64_t output_size,                                                                  \
+          const int64_t index_size,                                                                   \
+          const int64_t data_size,                                                                    \
+          const InType* input,                                                                        \
+          const IndexType* indices,                                                                   \
+          const int64_t* offsets,                                                                     \
+          const float* weights,                                                                       \
+          const double* scales,                                                                    \
+          bool normalize_by_lengths,                                                                  \
+          OutType* out) {                                                                             \
+    if (std::is_same<InType, int8_t>::value) {                                                       \
+      CAFFE_ENFORCE(scales != nullptr, "scales must not be nullptr");                         \
+    } else {                                                                                          \
+      CAFFE_ENFORCE(scales == nullptr, "scales must be nullptr");                             \
+    }                                                                                                 \
+    AVX2_FMA_DO(                                                                                      \
+        pt_EmbeddingLookupIdx_##IndexType##_##InTypeName##_##OutType##_##IS_WEIGHT_POSITIONAL,           \
+        block_size,                                                                                   \
+        output_size,                                                                                  \
+        index_size,                                                                                   \
+        data_size,                                                                                    \
+        input,                                                                                        \
+        indices,                                                                                      \
+        offsets,                                                                                      \
+        weights,                                                                                      \
+        scales,                                                                                   \
+        normalize_by_lengths,                                                                         \
+        out);                                                                                         \
+    BASE_DO(                                                                                          \
+        pt_EmbeddingLookupIdx_##IndexType##_##InTypeName##_##OutType##_##IS_WEIGHT_POSITIONAL,           \
+        block_size,                                                                                   \
+        output_size,                                                                                  \
+        index_size,                                                                                   \
+        data_size,                                                                                    \
+        input,                                                                                        \
+        indices,                                                                                      \
+        offsets,                                                                                      \
+        weights,                                                                                      \
+        scales,                                                                                   \
+        normalize_by_lengths,                                                                         \
+        out);                                                                                         \
+  }                                                                                                   \
+  template <>                                                                                         \
+  void pt_EmbeddingLookupIdx<IndexType, InType, OutType, IS_WEIGHT_POSITIONAL>(                          \
+      const int64_t block_size,                                                                       \
+      const int64_t output_size,                                                                      \
+      const int64_t index_size,                                                                       \
+      const int64_t data_size,                                                                        \
+      const InType* input,                                                                            \
+      const IndexType* indices,                                                                       \
+      const int64_t* offsets,                                                                         \
+      const float* weights,                                                                           \
+      const double* scales,                                                                        \
+      bool normalize_by_lengths,                                                                      \
+      OutType* out) {                                                                                 \
+    bool success =                                                                                    \
+        pt_EmbeddingLookupIdx_##IndexType##_##InTypeName##_##OutType##_##IS_WEIGHT_POSITIONAL(           \
+            block_size,                                                                               \
+            output_size,                                                                              \
+            index_size,                                                                               \
+            data_size,                                                                                \
+            input,                                                                                    \
+            indices,                                                                                  \
+            offsets,                                                                                  \
+            weights,                                                                                  \
+            scales,                                                                               \
+            normalize_by_lengths,                                                                     \
+            out);                                                                                     \
+    if (success) {                                                                                    \
+      return;                                                                                         \
+    }                                                                                                 \
+    int64_t current = 0;                                                                              \
+    for (int m = 0; m < output_size; ++m) {                                                           \
+      for (int64_t i = offsets[m];                                                                    \
+           i < (m == output_size - 1 ? index_size : offsets[m + 1]);                                  \
+           ++i) {                                                                                     \
+        CAFFE_ENFORCE_LT(current, index_size);                                                        \
+        IndexType idx = indices[current];                                                             \
+        CAFFE_ENFORCE(                                                                                \
+            0 <= idx && idx < data_size,                                                              \
+            "Index ",                                                                                 \
+            current,                                                                                  \
+            " is out of bounds: ",                                                                    \
+            idx,                                                                                      \
+            ", range 0 to ",                                                                          \
+            data_size);                                                                               \
+        ++current;                                                                                    \
+      }                                                                                               \
+    }                                                                                                 \
+    CAFFE_ENFORCE_EQ(                                                                                 \
+        current,                                                                                      \
+        index_size,                                                                                   \
+        "Your input seems to be incorrect: the sum of lengths values should be "                      \
+        "the size of the indices tensor, but it appears not.");                                       \
+  }
+
+
+PT_EMBEDDING_SPECIALIZATION(int64_t, int8_t, int8_t, float, false);
+
+
+PT_EMBEDDING_SPECIALIZATION(int64_t, int8_t, int8_t, float, true);
+
+#undef PT_EMBEDDING_SPECIALIZATION
 } // namespace caffe2
diff --git a/caffe2/perfkernels/embedding_lookup_idx.h b/caffe2/perfkernels/embedding_lookup_idx.h
index 9092b27..817e269 100644
--- a/caffe2/perfkernels/embedding_lookup_idx.h
+++ b/caffe2/perfkernels/embedding_lookup_idx.h
@@ -54,4 +54,25 @@ void EmbeddingLookupIdx(
     bool normalize_by_lengths,
     OutType* out);
 
+
+//different from caffe2, pytorch quantized embedding bag get scales for "double" type
+template <
+    typename IndexType,
+    typename InType,
+    typename OutType,
+    bool IS_WEIGHT_POSITIONAL = false>
+void pt_EmbeddingLookupIdx(
+    const std::int64_t block_size,
+    const std::int64_t output_size,
+    const std::int64_t index_size,
+    const std::int64_t data_size,
+    const InType* input,
+    const IndexType* indices,
+    const int64_t* offsets,
+    const float* weights, // optional, can be null for non-weighted sum
+    const double* scales, // scale params for int8 input
+    bool normalize_by_lengths,
+    OutType* out);
+
+
 } // namespace caffe2
diff --git a/caffe2/perfkernels/embedding_lookup_idx_avx2.cc b/caffe2/perfkernels/embedding_lookup_idx_avx2.cc
index eb61353..a78face 100644
--- a/caffe2/perfkernels/embedding_lookup_idx_avx2.cc
+++ b/caffe2/perfkernels/embedding_lookup_idx_avx2.cc
@@ -3137,4 +3137,569 @@ bool EmbeddingLookupIdx_int64_t_uint8_t_float_true__avx2_fma(
       out);
 }
 
+template <bool IS_WEIGHT_POSITIONAL>
+static bool pt_EmbeddingLookupIdx_int64_t_int8_t_float__avx2_fma(
+    const int64_t block_size,
+    const int64_t output_size,
+    const int64_t index_size,
+    const int64_t data_size,
+    const int8_t* input,
+    const int64_t* indices,
+    const int64_t* offsets,
+    const float* weights,
+    const double* scales,
+    bool normalize_by_lengths,
+    float* out) {
+  const int64_t prefdist_T0 = 16;
+  const int64_t fused_block_size = block_size + 0;
+  int64_t dataInd = 0;
+  if (block_size == 128) {
+    // unrolling 16 times
+    for (int64_t rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {
+      float* op = &out[rangeIndex * block_size];
+      __m256 vop0 = _mm256_setzero_ps();
+      __m256 vop8 = _mm256_setzero_ps();
+      __m256 vop16 = _mm256_setzero_ps();
+      __m256 vop24 = _mm256_setzero_ps();
+      __m256 vop32 = _mm256_setzero_ps();
+      __m256 vop40 = _mm256_setzero_ps();
+      __m256 vop48 = _mm256_setzero_ps();
+      __m256 vop56 = _mm256_setzero_ps();
+      __m256 vop64 = _mm256_setzero_ps();
+      __m256 vop72 = _mm256_setzero_ps();
+      __m256 vop80 = _mm256_setzero_ps();
+      __m256 vop88 = _mm256_setzero_ps();
+      __m256 vop96 = _mm256_setzero_ps();
+      __m256 vop104 = _mm256_setzero_ps();
+      __m256 vop112 = _mm256_setzero_ps();
+      __m256 vop120 = _mm256_setzero_ps();
+      if (dataInd != offsets[rangeIndex] - offsets[0]) {
+        return false;
+      }
+      int64_t end_offset = offsets[rangeIndex + 1];
+      int64_t length = end_offset - offsets[rangeIndex];
+      for (int64_t start = dataInd; dataInd < end_offset - offsets[0];
+           ++dataInd) {
+        const int64_t idx = indices[dataInd];
+        if (idx < 0 || idx >= data_size) {
+          return false;
+        }
+        float wgt = 1.f;
+        if (weights) {
+          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];
+        }
+        wgt = wgt * static_cast<float>(scales[idx]);
+        __m256 vwgt = _mm256_set1_ps(wgt);
+        const int8_t* ip = &input[idx * fused_block_size];
+        const int64_t next_T0 = (dataInd < index_size - prefdist_T0)
+            ? (dataInd + prefdist_T0)
+            : dataInd;
+        const int64_t idx_pref_T0 = indices[next_T0];
+          _mm_prefetch(
+              reinterpret_cast<const char*>(&scales[idx_pref_T0]), _MM_HINT_T0);
+        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {
+          return false;
+        }
+        const int8_t* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
+        vop0 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (0))))),
+            vop0);
+        _mm_prefetch(
+            reinterpret_cast<const char*>(&ip_next_T0[0]), _MM_HINT_T0);
+        vop8 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (8))))),
+            vop8);
+        // skip unnecessary prefetch of (&ip_next_T0[8])
+        vop16 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (16))))),
+            vop16);
+        // skip unnecessary prefetch of (&ip_next_T0[16])
+        vop24 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (24))))),
+            vop24);
+        // skip unnecessary prefetch of (&ip_next_T0[24])
+        vop32 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (32))))),
+            vop32);
+        // skip unnecessary prefetch of (&ip_next_T0[32])
+        vop40 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (40))))),
+            vop40);
+        // skip unnecessary prefetch of (&ip_next_T0[40])
+        vop48 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (48))))),
+            vop48);
+        // skip unnecessary prefetch of (&ip_next_T0[48])
+        vop56 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (56))))),
+            vop56);
+        // skip unnecessary prefetch of (&ip_next_T0[56])
+        vop64 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (64))))),
+            vop64);
+        _mm_prefetch(
+            reinterpret_cast<const char*>(&ip_next_T0[64]), _MM_HINT_T0);
+        vop72 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (72))))),
+            vop72);
+        // skip unnecessary prefetch of (&ip_next_T0[72])
+        vop80 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (80))))),
+            vop80);
+        // skip unnecessary prefetch of (&ip_next_T0[80])
+        vop88 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (88))))),
+            vop88);
+        // skip unnecessary prefetch of (&ip_next_T0[88])
+        vop96 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (96))))),
+            vop96);
+        // skip unnecessary prefetch of (&ip_next_T0[96])
+        vop104 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (104))))),
+            vop104);
+        // skip unnecessary prefetch of (&ip_next_T0[104])
+        vop112 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (112))))),
+            vop112);
+        // skip unnecessary prefetch of (&ip_next_T0[112])
+        vop120 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (120))))),
+            vop120);
+        // skip unnecessary prefetch of (&ip_next_T0[120])
+      }
+      if (!normalize_by_lengths || length == 0) {
+        _mm256_storeu_ps(&op[0], vop0);
+        _mm256_storeu_ps(&op[8], vop8);
+        _mm256_storeu_ps(&op[16], vop16);
+        _mm256_storeu_ps(&op[24], vop24);
+        _mm256_storeu_ps(&op[32], vop32);
+        _mm256_storeu_ps(&op[40], vop40);
+        _mm256_storeu_ps(&op[48], vop48);
+        _mm256_storeu_ps(&op[56], vop56);
+        _mm256_storeu_ps(&op[64], vop64);
+        _mm256_storeu_ps(&op[72], vop72);
+        _mm256_storeu_ps(&op[80], vop80);
+        _mm256_storeu_ps(&op[88], vop88);
+        _mm256_storeu_ps(&op[96], vop96);
+        _mm256_storeu_ps(&op[104], vop104);
+        _mm256_storeu_ps(&op[112], vop112);
+        _mm256_storeu_ps(&op[120], vop120);
+      } else {
+        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);
+        _mm256_storeu_ps(&op[0], _mm256_mul_ps(vop0, vlen_inv));
+        _mm256_storeu_ps(&op[8], _mm256_mul_ps(vop8, vlen_inv));
+        _mm256_storeu_ps(&op[16], _mm256_mul_ps(vop16, vlen_inv));
+        _mm256_storeu_ps(&op[24], _mm256_mul_ps(vop24, vlen_inv));
+        _mm256_storeu_ps(&op[32], _mm256_mul_ps(vop32, vlen_inv));
+        _mm256_storeu_ps(&op[40], _mm256_mul_ps(vop40, vlen_inv));
+        _mm256_storeu_ps(&op[48], _mm256_mul_ps(vop48, vlen_inv));
+        _mm256_storeu_ps(&op[56], _mm256_mul_ps(vop56, vlen_inv));
+        _mm256_storeu_ps(&op[64], _mm256_mul_ps(vop64, vlen_inv));
+        _mm256_storeu_ps(&op[72], _mm256_mul_ps(vop72, vlen_inv));
+        _mm256_storeu_ps(&op[80], _mm256_mul_ps(vop80, vlen_inv));
+        _mm256_storeu_ps(&op[88], _mm256_mul_ps(vop88, vlen_inv));
+        _mm256_storeu_ps(&op[96], _mm256_mul_ps(vop96, vlen_inv));
+        _mm256_storeu_ps(&op[104], _mm256_mul_ps(vop104, vlen_inv));
+        _mm256_storeu_ps(&op[112], _mm256_mul_ps(vop112, vlen_inv));
+        _mm256_storeu_ps(&op[120], _mm256_mul_ps(vop120, vlen_inv));
+      }
+    }
+  } else if (block_size == 64) {
+    // unrolling 8 times
+    for (int64_t rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {
+      float* op = &out[rangeIndex * block_size];
+      __m256 vop0 = _mm256_setzero_ps();
+      __m256 vop8 = _mm256_setzero_ps();
+      __m256 vop16 = _mm256_setzero_ps();
+      __m256 vop24 = _mm256_setzero_ps();
+      __m256 vop32 = _mm256_setzero_ps();
+      __m256 vop40 = _mm256_setzero_ps();
+      __m256 vop48 = _mm256_setzero_ps();
+      __m256 vop56 = _mm256_setzero_ps();
+      if (dataInd != offsets[rangeIndex] - offsets[0]) {
+        return false;
+      }
+      int64_t end_offset = offsets[rangeIndex + 1];
+      int64_t length = end_offset - offsets[rangeIndex];
+      for (int64_t start = dataInd; dataInd < end_offset - offsets[0];
+           ++dataInd) {
+        const int64_t idx = indices[dataInd];
+        if (idx < 0 || idx >= data_size) {
+          return false;
+        }
+        float wgt = 1.f;
+        if (weights) {
+          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];
+        }
+        wgt = wgt * static_cast<float>(scales[idx]);
+        __m256 vwgt = _mm256_set1_ps(wgt);
+        const int8_t* ip = &input[idx * fused_block_size];
+        const int64_t next_T0 = (dataInd < index_size - prefdist_T0)
+            ? (dataInd + prefdist_T0)
+            : dataInd;
+        const int64_t idx_pref_T0 = indices[next_T0];
+          _mm_prefetch(
+              reinterpret_cast<const char*>(&scales[idx_pref_T0]), _MM_HINT_T0);
+        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {
+          return false;
+        }
+        const int8_t* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
+        vop0 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (0))))),
+            vop0);
+        _mm_prefetch(
+            reinterpret_cast<const char*>(&ip_next_T0[0]), _MM_HINT_T0);
+        vop8 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (8))))),
+            vop8);
+        // skip unnecessary prefetch of (&ip_next_T0[8])
+        vop16 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (16))))),
+            vop16);
+        // skip unnecessary prefetch of (&ip_next_T0[16])
+        vop24 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (24))))),
+            vop24);
+        // skip unnecessary prefetch of (&ip_next_T0[24])
+        vop32 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (32))))),
+            vop32);
+        // skip unnecessary prefetch of (&ip_next_T0[32])
+        vop40 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (40))))),
+            vop40);
+        // skip unnecessary prefetch of (&ip_next_T0[40])
+        vop48 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (48))))),
+            vop48);
+        // skip unnecessary prefetch of (&ip_next_T0[48])
+        vop56 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (56))))),
+            vop56);
+        // skip unnecessary prefetch of (&ip_next_T0[56])
+      }
+      if (!normalize_by_lengths || length == 0) {
+        _mm256_storeu_ps(&op[0], vop0);
+        _mm256_storeu_ps(&op[8], vop8);
+        _mm256_storeu_ps(&op[16], vop16);
+        _mm256_storeu_ps(&op[24], vop24);
+        _mm256_storeu_ps(&op[32], vop32);
+        _mm256_storeu_ps(&op[40], vop40);
+        _mm256_storeu_ps(&op[48], vop48);
+        _mm256_storeu_ps(&op[56], vop56);
+      } else {
+        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);
+        _mm256_storeu_ps(&op[0], _mm256_mul_ps(vop0, vlen_inv));
+        _mm256_storeu_ps(&op[8], _mm256_mul_ps(vop8, vlen_inv));
+        _mm256_storeu_ps(&op[16], _mm256_mul_ps(vop16, vlen_inv));
+        _mm256_storeu_ps(&op[24], _mm256_mul_ps(vop24, vlen_inv));
+        _mm256_storeu_ps(&op[32], _mm256_mul_ps(vop32, vlen_inv));
+        _mm256_storeu_ps(&op[40], _mm256_mul_ps(vop40, vlen_inv));
+        _mm256_storeu_ps(&op[48], _mm256_mul_ps(vop48, vlen_inv));
+        _mm256_storeu_ps(&op[56], _mm256_mul_ps(vop56, vlen_inv));
+      }
+    }
+  } else if (block_size == 32) {
+    // unrolling 4 times
+    for (int64_t rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {
+      float* op = &out[rangeIndex * block_size];
+      __m256 vop0 = _mm256_setzero_ps();
+      __m256 vop8 = _mm256_setzero_ps();
+      __m256 vop16 = _mm256_setzero_ps();
+      __m256 vop24 = _mm256_setzero_ps();
+      if (dataInd != offsets[rangeIndex] - offsets[0]) {
+        return false;
+      }
+      int64_t end_offset = offsets[rangeIndex + 1];
+      int64_t length = end_offset - offsets[rangeIndex];
+      for (int64_t start = dataInd; dataInd < end_offset - offsets[0];
+           ++dataInd) {
+        const int64_t idx = indices[dataInd];
+        if (idx < 0 || idx >= data_size) {
+          return false;
+        }
+        float wgt = 1.f;
+        if (weights) {
+          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];
+        }
+        wgt = wgt * static_cast<float>(scales[idx]);
+        __m256 vwgt = _mm256_set1_ps(wgt);
+        const int8_t* ip = &input[idx * fused_block_size];
+        const int64_t next_T0 = (dataInd < index_size - prefdist_T0)
+            ? (dataInd + prefdist_T0)
+            : dataInd;
+        const int64_t idx_pref_T0 = indices[next_T0];
+          _mm_prefetch(
+              reinterpret_cast<const char*>(&scales[idx_pref_T0]), _MM_HINT_T0);
+        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {
+          return false;
+        }
+        const int8_t* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
+        vop0 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (0))))),
+            vop0);
+        _mm_prefetch(
+            reinterpret_cast<const char*>(&ip_next_T0[0]), _MM_HINT_T0);
+        vop8 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (8))))),
+            vop8);
+        // skip unnecessary prefetch of (&ip_next_T0[8])
+        vop16 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (16))))),
+            vop16);
+        // skip unnecessary prefetch of (&ip_next_T0[16])
+        vop24 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (24))))),
+            vop24);
+        // skip unnecessary prefetch of (&ip_next_T0[24])
+      }
+      if (!normalize_by_lengths || length == 0) {
+        _mm256_storeu_ps(&op[0], vop0);
+        _mm256_storeu_ps(&op[8], vop8);
+        _mm256_storeu_ps(&op[16], vop16);
+        _mm256_storeu_ps(&op[24], vop24);
+      } else {
+        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);
+        _mm256_storeu_ps(&op[0], _mm256_mul_ps(vop0, vlen_inv));
+        _mm256_storeu_ps(&op[8], _mm256_mul_ps(vop8, vlen_inv));
+        _mm256_storeu_ps(&op[16], _mm256_mul_ps(vop16, vlen_inv));
+        _mm256_storeu_ps(&op[24], _mm256_mul_ps(vop24, vlen_inv));
+      }
+    }
+  } else if (block_size == 16) {
+    // unrolling 2 times
+    for (int64_t rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {
+      float* op = &out[rangeIndex * block_size];
+      __m256 vop0 = _mm256_setzero_ps();
+      __m256 vop8 = _mm256_setzero_ps();
+      if (dataInd != offsets[rangeIndex] - offsets[0]) {
+        return false;
+      }
+      int64_t end_offset = offsets[rangeIndex + 1];
+      int64_t length = end_offset - offsets[rangeIndex];
+      for (int64_t start = dataInd; dataInd < end_offset - offsets[0];
+           ++dataInd) {
+        const int64_t idx = indices[dataInd];
+        if (idx < 0 || idx >= data_size) {
+          return false;
+        }
+        float wgt = 1.f;
+        if (weights) {
+          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];
+        }
+        wgt = wgt * static_cast<float>(scales[idx]);
+        __m256 vwgt = _mm256_set1_ps(wgt);
+        const int8_t* ip = &input[idx * fused_block_size];
+        const int64_t next_T0 = (dataInd < index_size - prefdist_T0)
+            ? (dataInd + prefdist_T0)
+            : dataInd;
+        const int64_t idx_pref_T0 = indices[next_T0];
+          _mm_prefetch(
+              reinterpret_cast<const char*>(&scales[idx_pref_T0]), _MM_HINT_T0);
+        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {
+          return false;
+        }
+        const int8_t* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
+        vop0 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (0))))),
+            vop0);
+        _mm_prefetch(
+            reinterpret_cast<const char*>(&ip_next_T0[0]), _MM_HINT_T0);
+        vop8 = _mm256_fmadd_ps(
+            vwgt,
+            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(
+                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (8))))),
+            vop8);
+        // skip unnecessary prefetch of (&ip_next_T0[8])
+      }
+      if (!normalize_by_lengths || length == 0) {
+        _mm256_storeu_ps(&op[0], vop0);
+        _mm256_storeu_ps(&op[8], vop8);
+      } else {
+        __m256 vlen_inv = _mm256_set1_ps(1.0f / length);
+        _mm256_storeu_ps(&op[0], _mm256_mul_ps(vop0, vlen_inv));
+        _mm256_storeu_ps(&op[8], _mm256_mul_ps(vop8, vlen_inv));
+      }
+    }
+  } else {
+    // generic code
+    for (int64_t rangeIndex = 0; rangeIndex < output_size; ++rangeIndex) {
+      float* op = &out[rangeIndex * block_size];
+      int64_t j = 0;
+      for (; j + 8 <= block_size; j += 8) {
+        _mm256_storeu_ps(op + j, _mm256_setzero_ps());
+      }
+      for (; j < block_size; j++) {
+        op[j] = 0.0f;
+      }
+      if (dataInd != offsets[rangeIndex] - offsets[0]) {
+        return false;
+      }
+      int64_t end_offset = offsets[rangeIndex + 1];
+      int64_t length = end_offset - offsets[rangeIndex];
+      for (int64_t start = dataInd; dataInd < end_offset - offsets[0];
+           ++dataInd) {
+        const int64_t idx = indices[dataInd];
+        if (idx < 0 || idx >= data_size) {
+          return false;
+        }
+        float wgt = 1.f;
+        if (weights) {
+          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];
+        }
+        wgt = wgt * static_cast<float>(scales[idx]);
+        __m256 vwgt = _mm256_set1_ps(wgt);
+        const int8_t* ip = &input[idx * fused_block_size];
+        const int64_t next_T0 = (dataInd < index_size - prefdist_T0)
+            ? (dataInd + prefdist_T0)
+            : dataInd;
+        const int64_t idx_pref_T0 = indices[next_T0];
+          _mm_prefetch(
+              reinterpret_cast<const char*>(&scales[idx_pref_T0]), _MM_HINT_T0);
+        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {
+          return false;
+        }
+        const int8_t* ip_next_T0 = &input[idx_pref_T0 * fused_block_size];
+        j = 0;
+        for (; j + 8 <= block_size; j += 8) {
+          _mm256_storeu_ps(
+              &op[j],
+              _mm256_fmadd_ps(
+                  vwgt,
+                  _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(_mm_loadl_epi64(
+                      reinterpret_cast<const __m128i*>(&ip[j])))),
+                  _mm256_loadu_ps(&op[j])));
+          _mm_prefetch(
+              reinterpret_cast<const char*>(&ip_next_T0[j]), _MM_HINT_T0);
+        }
+        for (; j < block_size; j++) {
+          op[j] = std::fma(wgt, (float)ip[j], op[j]);
+        }
+      }
+      if (normalize_by_lengths && length) {
+        float len_inv = 1.0f / length;
+        __m256 vlen_inv = _mm256_set1_ps(len_inv);
+        j = 0;
+        for (; j + 8 <= block_size; j += 8) {
+          _mm256_storeu_ps(
+              &op[j], _mm256_mul_ps(_mm256_loadu_ps(&op[j]), vlen_inv));
+        }
+        for (; j < block_size; j++) {
+          op[j] = len_inv * op[j];
+        }
+      }
+    }
+  }
+  return dataInd == index_size;
+}
+bool pt_EmbeddingLookupIdx_int64_t_int8_t_float_false__avx2_fma(
+    const int64_t block_size,
+    const int64_t output_size,
+    const int64_t index_size,
+    const int64_t data_size,
+    const int8_t* input,
+    const int64_t* indices,
+    const int64_t* offsets,
+    const float* weights,
+    const double* scales,
+    bool normalize_by_lengths,
+    float* out) {
+  return pt_EmbeddingLookupIdx_int64_t_int8_t_float__avx2_fma<false>(
+      block_size,
+      output_size,
+      index_size,
+      data_size,
+      input,
+      indices,
+      offsets,
+      weights,
+      scales,
+      normalize_by_lengths,
+      out);
+}
+bool pt_EmbeddingLookupIdx_int64_t_int8_t_float_true__avx2_fma(
+    const int64_t block_size,
+    const int64_t output_size,
+    const int64_t index_size,
+    const int64_t data_size,
+    const int8_t* input,
+    const int64_t* indices,
+    const int64_t* offsets,
+    const float* weights,
+    const double* scales,
+    bool normalize_by_lengths,
+    float* out) {
+  return pt_EmbeddingLookupIdx_int64_t_int8_t_float__avx2_fma<true>(
+      block_size,
+      output_size,
+      index_size,
+      data_size,
+      input,
+      indices,
+      offsets,
+      weights,
+      scales,
+      normalize_by_lengths,
+      out);
+}
+
 } // namespace caffe2
diff --git a/caffe2/perfkernels/hp_emblookup_codegen.py b/caffe2/perfkernels/hp_emblookup_codegen.py
index f79b7c8..8353564 100644
--- a/caffe2/perfkernels/hp_emblookup_codegen.py
+++ b/caffe2/perfkernels/hp_emblookup_codegen.py
@@ -4,7 +4,7 @@ import argparse
 import sys
 
 
-sizeof = {"float": 4, "at::Half": 2, "uint8_t": 1}
+sizeof = {"float": 4, "at::Half": 2, "uint8_t": 1, "int8_t": 1}
 
 
 def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets):
@@ -32,6 +32,14 @@ def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets)
                 "                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\n"  # noqa
                 "            _mm256_add_ps(vop%d, vbio));" % (regid, regid, regid)
             )
+        elif InType == "int8_t":
+            code.append(
+                "        vop%d = _mm256_fmadd_ps(\n"
+                "            vwgt,\n"
+                "            _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(\n"
+                "                _mm_loadl_epi64(reinterpret_cast<const __m128i*>(ip + (%d))))),\n"  # noqa
+                "            vop%d);" % (regid, regid, regid)
+            )
         else:
             assert False
 
@@ -121,6 +129,14 @@ def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets)
             code.append("        bio = wgt * scale_bias[2 * idx + 1];")
             code.append("        wgt = wgt * scale_bias[2 * idx];")
         code.append("        __m256 vbio = _mm256_set1_ps(bio);")
+    elif InType == "int8_t":
+        code.append("        " + OutType + " wgt = 1.f;")
+        code.append("        if (weights) {")
+        code.append(
+            "          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];"  # noqa
+        )
+        code.append("        }")
+        code.append("        wgt = wgt * static_cast<float>(scales[idx]);")
     else:
         code.append("        " + OutType + " wgt = 1.f;")
         code.append("        if (weights) {")
@@ -138,6 +154,11 @@ def unroll(uf, IndexType, InType, OutType, use_weights, isa, fused, use_offsets)
         )
     )
     code.append("        const " + IndexType + " idx_pref_T0 = indices[next_T0];")
+    if InType == "int8_t":
+        code.append(
+            "          _mm_prefetch(\n"
+            "              reinterpret_cast<const char*>(&scales[idx_pref_T0]), _MM_HINT_T0);"
+        )
     code.append(
         "        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\n"
         + "          return false;\n"
@@ -216,6 +237,16 @@ def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):
                 "                      reinterpret_cast<const __m128i*>(&ip[j])))),\n"
                 "                  _mm256_add_ps(_mm256_loadu_ps(&op[j]), vbio)));"
             )
+        elif InType == "int8_t":
+            code.append(
+                "          _mm256_storeu_ps(\n"
+                "              &op[j],\n"
+                "              _mm256_fmadd_ps(\n"
+                "                  vwgt,\n"
+                "                  _mm256_cvtepi32_ps(_mm256_cvtepi8_epi32(_mm_loadl_epi64(\n"  # noqa
+                "                      reinterpret_cast<const __m128i*>(&ip[j])))),\n"
+                "                  _mm256_loadu_ps(&op[j])));"
+            )
         else:
             assert False
 
@@ -308,6 +339,14 @@ def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):
             code.append("        bio = wgt * scale_bias[2 * idx + 1];")
             code.append("        wgt = wgt * scale_bias[2 * idx];")
         code.append("        __m256 vbio = _mm256_set1_ps(bio);")
+    elif InType == "int8_t":
+        code.append("        " + OutType + " wgt = 1.f;")
+        code.append("        if (weights) {")
+        code.append(
+            "          wgt = weights[IS_WEIGHT_POSITIONAL ? (dataInd - start) : dataInd];"  # noqa
+        )
+        code.append("        }")
+        code.append("        wgt = wgt * static_cast<float>(scales[idx]);")
     else:
         code.append("        " + OutType + " wgt = 1.f;")
         code.append("        if (weights) {")
@@ -325,6 +364,11 @@ def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):
         )
     )
     code.append("        const " + IndexType + " idx_pref_T0 = indices[next_T0];")
+    if InType == "int8_t":
+        code.append(
+            "          _mm_prefetch(\n"
+            "              reinterpret_cast<const char*>(&scales[idx_pref_T0]), _MM_HINT_T0);"
+        )
     code.append(
         "        if (idx_pref_T0 < 0 || idx_pref_T0 >= data_size) {\n"
         + "          return false;\n"
@@ -353,6 +397,8 @@ def generic(IndexType, InType, OutType, use_weights, isa, fused, use_offsets):
         code.append("          op[j] = std::fma(wgt, ((float*)(&vtmp2))[0], op[j]);")
     elif InType == "uint8_t":
         code.append("          op[j] = std::fma(wgt, (float)ip[j], bio + op[j]);")
+    elif InType == "int8_t":
+        code.append("          op[j] = std::fma(wgt, (float)ip[j], op[j]);")
     else:
         assert False
 
@@ -412,6 +458,8 @@ options = [
     ["int64_t", "int64_t", "uint8_t", "uint8_t", "float", "float"],
 ]
 
+if filename == "embedding_lookup_idx_avx2.cc":
+    options.append(["int64_t", "int64_t", "int8_t", "int8_t", "float", "float"])
 code = []
 # includes
 code.append("//// --------------------------")
@@ -429,6 +477,8 @@ for o in options:
     [IndexTypeName, IndexType, InTypeName, InType, OutTypeName, OutType] = o
 
     prefix = "Fused8BitRowwise" if opts.fused else ""
+    if InType == "int8_t":
+        prefix = "pt_"
     code.append("template <bool IS_WEIGHT_POSITIONAL>")
     if opts.use_offsets:
         fn_base = "{}EmbeddingLookupIdx_{}_{}_{}".format(
@@ -455,7 +505,10 @@ for o in options:
         args.append("    const int* lengths,")
     args.append("    const float* weights,")
     if not opts.fused:
-        args.append("    const float* scale_bias,")
+        if InType == "int8_t":
+            args.append("    const double* scales,")
+        else:
+            args.append("    const float* scale_bias,")
     args.append("    bool normalize_by_lengths,")
     args.append("    " + OutType + "* out) {")
     code += args
@@ -512,7 +565,10 @@ for o in options:
             code.append("      lengths,")
         code.append("      weights,")
         if not opts.fused:
-            code.append("      scale_bias,")
+            if InType == "int8_t":
+                code.append("      scales,")
+            else:
+                code.append("      scale_bias,")
         code.append("      normalize_by_lengths,")
         code.append("      out);")
         code.append("}")
diff --git a/torch/nn/quantized/modules/__init__.py b/torch/nn/quantized/modules/__init__.py
index b3894fa..5880fbe 100644
--- a/torch/nn/quantized/modules/__init__.py
+++ b/torch/nn/quantized/modules/__init__.py
@@ -7,6 +7,7 @@ from .activation import ReLU, ReLU6
 from .batchnorm import BatchNorm2d, BatchNorm3d
 from .conv import Conv2d, Conv3d
 from .linear import Linear
+from .emb import EmbeddingBag
 
 from .functional_modules import FloatFunctional, QFunctional
 
@@ -51,6 +52,23 @@ class Quantize(torch.nn.Module):
     def extra_repr(self):
         return 'scale={}, zero_point={}, dtype={}'.format(self.scale, self.zero_point, self.dtype)
 
+    def _save_to_state_dict(self, destination, prefix, keep_vars):
+        destination[prefix + 'scale'] = self.scale
+        destination[prefix + 'zero_point'] = self.zero_point
+        destination[prefix + 'dtype'] = self.dtype
+
+    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
+    missing_keys, unexpected_keys, error_msgs):
+        self.scale = torch.tensor(state_dict[prefix + 'scale'],dtype=torch.float)
+        state_dict.pop(prefix + 'scale')
+
+        self.zero_point = torch.tensor((state_dict[prefix + 'zero_point']), dtype=torch.long)
+        state_dict.pop(prefix + 'zero_point')
+
+        self.dtype = (state_dict[prefix + 'dtype'])
+        state_dict.pop(prefix + 'dtype')
+
+
 
 class DeQuantize(torch.nn.Module):
     r"""Dequantizes an incoming tensor
@@ -86,6 +104,7 @@ __all__ = [
     'Linear',
     'MaxPool2d',
     'Quantize',
+    'EmbeddingBag',
     'ReLU',
     'ReLU6',
     # Wrapper modules
diff --git a/torch/nn/quantized/modules/emb.py b/torch/nn/quantized/modules/emb.py
new file mode 100644
index 0000000..b76efd5
--- /dev/null
+++ b/torch/nn/quantized/modules/emb.py
@@ -0,0 +1,61 @@
+r"""Quantized EmbeddingBag modules."""
+
+import torch
+import torch.nn as nn
+from ... import functional as F
+
+
+class EmbeddingBag(nn.Module):
+    _FLOAT_MODULE = nn.EmbeddingBag
+
+    def __init__(self, num_embeddings, embedding_dim,
+                 max_norm=None, norm_type=2., scale_grad_by_freq=False,
+                 mode='mean', sparse=False, _weight=None):
+        super(EmbeddingBag, self).__init__()
+        self.num_embeddings = num_embeddings
+        self.embedding_dim = embedding_dim
+        self.max_norm = max_norm
+        self.norm_type = norm_type
+        self.scale_grad_by_freq = scale_grad_by_freq
+        self.mode = mode
+        self.sparse = sparse
+
+        # TODO: per channel quantization
+        if _weight is None:
+            self.register_buffer('weight', torch._empty_affine_quantized(
+                [num_embeddings, embedding_dim], scale=1, zero_point=0, dtype=torch.qint8))
+        else:
+            self.weight = _weight
+
+    def forward(self, input, offsets=None, per_sample_weights=None):
+        # type: (Tensor, Optional[Tensor], Optional[Tensor]) -> Tensor
+        return F.embedding_bag(input, self.weight, offsets,
+                               self.max_norm, self.norm_type,
+                               self.scale_grad_by_freq, self.mode, self.sparse,
+                               per_sample_weights)
+
+    @classmethod
+    def from_float(cls, mod):
+        assert type(mod) == cls._FLOAT_MODULE, ' nnq.' + cls.__name__ + '.from_float only works for ' + \
+            cls._FLOAT_MODULE.__name__
+        weight_observer = mod.qconfig.weight()
+        weight_observer(mod.weight)
+        assert weight_observer.dtype == torch.qint8, 'Weight observer must have a dtype of qint8'
+        wt_scale, wt_zp = weight_observer.calculate_qparams()
+        assert len(wt_scale) == mod.weight.size()[0], 'only support per_channel_quantization now'
+        qweight = torch.quantize_per_channel(mod.weight.float(), wt_scale, wt_zp, 0,
+                                                    torch.qint8)
+        qemb = cls(mod.num_embeddings, mod.embedding_dim, mod.max_norm, mod.norm_type,
+                   mod.scale_grad_by_freq, mod.mode, mod.sparse, None)
+        qemb.weight = qweight
+        return qemb
+
+    def _save_to_state_dict(self, destination, prefix, keep_vars):
+        super(EmbeddingBag, self)._save_to_state_dict(destination, prefix, keep_vars)
+
+
+    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,
+                              missing_keys, unexpected_keys, error_msgs):
+        super(EmbeddingBag, self)._load_from_state_dict(state_dict, prefix, local_metadata, False,
+                                                missing_keys, unexpected_keys, error_msgs)
+
diff --git a/torch/quantization/default_mappings.py b/torch/quantization/default_mappings.py
index 4ff90b7..067c067 100644
--- a/torch/quantization/default_mappings.py
+++ b/torch/quantization/default_mappings.py
@@ -17,6 +17,7 @@ DEFAULT_MODULE_MAPPING = {
     nn.ReLU6: nnq.ReLU6,
     nn.Conv2d: nnq.Conv2d,
     nn.Conv3d: nnq.Conv3d,
+    nn.EmbeddingBag: nnq.EmbeddingBag,
     nn.BatchNorm2d: nnq.BatchNorm2d,
     nn.BatchNorm3d: nnq.BatchNorm3d,
     QuantStub: nnq.Quantize,
